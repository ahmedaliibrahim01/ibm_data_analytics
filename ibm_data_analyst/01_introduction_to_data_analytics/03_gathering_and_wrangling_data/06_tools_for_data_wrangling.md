# Tools for Data Wrangling

In this video, we will look at some of the popularly used data wrangling software and tools, such as: Excel Power Query / Spreadsheets, OpenRefine, Google DataPrep, Watson Studio Refinery, Trifacta Wrangler, Python and R. Let’s begin with the most basic software used for manual wrangling—Spreadsheets. 

Spreadsheets such as Microsoft Excel and Google Sheets have a host of features and in-built formulae that can help you identify issues, clean, and transform data. 

Add-ins are available that allow you to import data from several different types of sources and clean and transform data as needed—such as Microsoft Power Query for Excel and Google Sheets Query function for Google Sheets. 

OpenRefine is an open-source tool that allows you to import and export data in a wide variety of formats, such as TSV, CSV, XLS, XML, and JSON. 

Using OpenRefine, you can clean data, transform it from one format to another, and extend data with web services and external data. 

OpenRefine is easy to learn and easy to use. 

It offers menu-based operations, which means you don’t need to memorize commands or syntax. 

Google DataPrep is an intelligent cloud data service that allows you to visually explore, clean, and prepare both structured and unstructured data for analysis. 

It is a fully managed service, which means you don’t need to install or manage the software or the infrastructure. DataPrep is extremely easy to use. 

With every action that you take, you get suggestions on what your ideal next step should be. 

DataPrep can automatically detect schemas, data types, and anomalies.

 Watson Studio Refinery, available via IBM Watson Studio, allows you to discover, cleanse, and transform data with built-in operations. 
 
 It transforms large amounts of raw data into consumable, quality information that’s ready for analytics. 
 
 Data Refinery offers the flexibility of exploring data residing in a spectrum of data sources. 
 
 It detects data types and classifications automatically and also enforces applicable data governance policies automatically.
 
  Trifacta Wrangler is an interactive cloud-based service for cleaning and transforming data.
 
 It takes messy, real-world data and cleans and rearranges it into data tables, which can then be exported to Excel, Tableau, and R. It is known for its collaboration features, allowing multiple team members to work simultaneously. 
 
 Python has a huge library and set of packages that offer powerful data manipulation capabilities. 
 
 Let’s look at a few of these libraries and packages. 
 
 Jupyter Notebook is an open-source web application widely used for data cleaning and transformation, statistical modeling, also data visualization. 
 
 Numpy, or Numerical Python, is the most basic package that Python offers. 
 
 It is fast, versatile, interoperable, and easy to use. 
 
 It provides support for large, multi-dimensional arrays and matrices, and high-level mathematical functions to operate on these arrays. 
 
 Pandas is designed for fast and easy data analysis operations. 
 
 It allows complex operations such as merging, joining, and transforming huge chunks of data, performed using simple, single-line commands. 
 
 Using Pandas, you can prevent common errors that result from misaligned data coming in from different sources. R, also offers a series of libraries and packages that are explicitly created for wrangling messy data—such as Dplyr, Data.table, and Jsonlite.
 
  Using these libraries, you can investigate, manipulate, and analyze data. Dplyr is a powerful library for data wrangling. 
  
  It has a precise and straightforward syntax. Data.table helps to aggregate large data sets quickly. 
  
  Jsonlite is a robust JSON parsing tool, great for interacting with web APIs. 
  
  Tools for data wrangling come with varying capabilities and dimensions. 
  
  Your decision regarding the best tool for your needs will depend on factors that are specific to your use case, infrastructure, and teams—such as supported data size, data structures, cleaning and transformation capabilities, infrastructure needs, ease of use, and learnability.