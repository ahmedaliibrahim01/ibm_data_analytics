# Viewpoints: Data Preparation and Reliability

In this segment, data professional share what portion of their job involves gathering, cleaning, and preparing data for analysis. 

I would say, a relatively big proportion of my job involves gathering, preparing, and cleaning data for analysis. 

I work at a company with a really great data engineering team. 

So I don't have to do this kind of work as much as some other data scientists do. 

But still, any person that is working closely with data, be they're a data scientist, a data analyst, machine learning engineer, really needs to get comfortable understanding where the data comes from. 

Inevitably, no dataset is perfect. 

There's always going to be compromises or small errors. 

So it's really important to spend a significant portion of your time, understanding the underlined data that was used to generate the dataset and what some potential problems might be with that data. 

My job as a CPA involves a lot of analysis. 

Financial statements, account activity, assessing processes, and controls. 

The gathering piece can be pretty simple as long as, the accounting information resides in a general ledger system or a central repository where the data is easy to gather. 

Probably, about 30 percent of the job is laying everything out. 

So when you get into analytics of it, you can just dive right into the meat and potatoes of it. 

So you need to track the data, make sure it's accurate, make sure things are adding up. 

Make sure you have all mumps of information. 

So for example, on financial statements, I need to make sure that people have given me 12 months of statements, I'm not missing any data and that if I am, that I have enough information to be able to project or to forecast or even look back to estimate what was done in the based on what I have. 

That is definitely helpful. 

In this segment, data professionals talk about the steps they take to ensure data is reliable. 

One of the essential steps to making sure your data is reliable, is to run summary statistics on individual columns in your data and make sure that they're consistent with reality. 

For example, if you have a column somewhere that records visits per month to a website and you run summary statistics on that column, you get the minimum, the mean, the median, the max, and you see something funky like, one month there's negative visits or something like this. 

You know, that data isn't reliable. 

Financial information in particular must be reliable. 

It must be non-bias. 

It must be free from error. 

Those are just a few of the many attributes that are necessary for data to be relied upon. 

So doing what I call a logic check before you get into the details of a transaction. 

Does it make sense at a high level? If you expected top-line revenue to increase, but you see that it has drastically decreased, then figure that part out first. 

Is my source correct? Am I running a query in the right period? Am I pulling the right general ledger account? So start there, make sure that basic data integrity questions have been addressed first. 

Once we know that the data is reliable, then we can start to deep dive into the reviews and form conclusions about the financial performance based on our analysis of the data.